##########################################################################
# SPARK BERT íŒŒë¼ë¯¸í„° ë¶„í¬ ë¶„ì„ ë° ì‹œê°í™”
# ë¶„ì„ ëŒ€ìƒ1 : BERT ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ë¶„í¬
# ë¶„ì„ ëŒ€ìƒ2 : SPARK Encoding í›„ ì†ì‹¤(Lossy)/ë¬´ì†ì‹¤(Lossless) êµ¬ê°„ í†µê³„
##########################################################################
# import torch
# import matplotlib.pyplot as plt
# import numpy as np
# from transformers import BertForSequenceClassification, BertTokenizer

# # ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
# model = BertForSequenceClassification.from_pretrained(
#     "textattack/bert-base-uncased-SST-2",
#     trust_remote_code=True,
#     use_safetensors=True
# )

# tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# # SPARK ì†ì‹¤ ê°€ëŠ¥ ì˜ì—­ ì •ì˜ (ë…¼ë¬¸ Table II ê¸°ì¤€)
# spark_lossy_range = set()
# for start in [16, 48, 80, 112, 128, 160, 192, 224]:
#     spark_lossy_range.update(range(start, start + 16))

# # ë ˆì´ì–´ë³„ ì²˜ë¦¬
# for name, param in model.named_parameters():
#     if "weight" not in name or not param.requires_grad:
#         continue

#     print(f"\nğŸ“Œ [Layer] {name}")

#     # (1) FP32 ë¶„í¬ ì‹œê°í™”
#     data_fp32 = param.data.cpu().view(-1).numpy()
#     plt.hist(data_fp32, bins=100, color='skyblue')
#     plt.title(f"{name} - FP32 Parameter Distribution")
#     plt.xlabel("Value")
#     plt.ylabel("Frequency")
#     plt.grid(True)
#     plt.tight_layout()
#     plt.show()

#     # (2) INT8 ì–‘ìí™”
#     max_val = param.data.abs().max()
#     scale = max_val / 127
#     int8_tensor = torch.round(param.data / scale).clamp(-128, 127).to(torch.int)
#     data_int8 = int8_tensor.cpu().view(-1).numpy()

#     plt.hist(data_int8, bins=256, range=(-128, 127), color='orange')
#     plt.title(f"{name} - INT8 Quantized Distribution")
#     plt.xlabel("Quantized Value")
#     plt.ylabel("Frequency")
#     plt.grid(True)
#     plt.tight_layout()
#     plt.show()

#     # (3) SPARK ì†ì‹¤ í†µê³„ ê³„ì‚°
#     quantized_tensor = int8_tensor + 128  # unsignedë¡œ ë³€í™˜
#     count_lossy = 0
#     count_safe = 0

#     for v in quantized_tensor.view(-1):
#         if int(v) in spark_lossy_range:
#             count_lossy += 1
#         else:
#             count_safe += 1

#     total = count_lossy + count_safe
#     print(f"   â–¶ Total Params     : {total}")
#     print(f"   â–¶ Lossy Range      : {count_lossy} ({100 * count_lossy / total:.2f}%)")
#     print(f"   â–¶ Lossless Range   : {count_safe} ({100 * count_safe / total:.2f}%)")

#     # (4) SPARK ì†ì‹¤ íˆìŠ¤í† ê·¸ë¨
#     plt.figure(figsize=(5, 4))
#     plt.bar(['Lossy', 'Lossless'], [count_lossy, count_safe], color=['red', 'green'])
#     plt.yscale('log')
#     plt.title(f"{name} - SPARK Encoding Zones (Log Scale)")
#     plt.ylabel("Log-scaled Number of Params")
#     plt.tight_layout()
#     plt.grid(axis='y', which='both')
#     plt.savefig(f"{name.replace('.', '_')}_spark_encoding_log_clean.png")
#     plt.close()



#     # âœ… í•œ ê°œ ë ˆì´ì–´ë§Œ ë³´ê³  ì‹¶ì„ ê²½ìš° break
#     break

import torch
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from transformers import BertForSequenceClassification

# ëª¨ë¸ ë¡œë”©
model = BertForSequenceClassification.from_pretrained(
    "textattack/bert-base-uncased-SST-2",
    trust_remote_code=True,
    use_safetensors=True
)

# íŠ¹ì • ë ˆì´ì–´ë§Œ ì„ íƒ
param = dict(model.named_parameters())["bert.embeddings.word_embeddings.weight"]
data_fp32 = param.data.cpu().view(-1).numpy()

# íˆìŠ¤í† ê·¸ë¨ + KDE (ì»¤ë„ ë°€ë„ ì¶”ì •) ê°™ì´ ì‹œê°í™”
plt.figure(figsize=(8, 4))
sns.histplot(data_fp32, bins=100, kde=True, color='blue')
plt.title("FP32 Weight Distribution - bert.embeddings.word_embeddings.weight")
plt.xlabel("Weight Value")
plt.ylabel("Frequency")
plt.grid(True)
plt.tight_layout()
plt.savefig("fp32_distribution_word_embedding.png")
plt.show()
